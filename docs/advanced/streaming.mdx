---
title: Advanced Streaming
description: Advanced streaming configuration for KoreShield.
slug: /features/streaming
---

# Advanced Streaming

KoreShield supports streaming responses through its OpenAI-compatible proxy. This page covers production-grade streaming guidance, timeouts, and infrastructure considerations.

## Use Cases

- Low-latency user experiences with partial tokens
- Long-form generation where full responses exceed typical timeouts
- Real-time dashboards and agent pipelines

## How Streaming Works

1. Client sends a request with `stream: true` to the KoreShield proxy.
2. KoreShield applies security checks, then forwards the request to the provider.
3. The proxy relays streamed chunks to the client as they arrive.

## Client Examples

### TypeScript (Fetch)

```typescript
const response = await fetch("http://localhost:8000/v1/chat/completions", {
  method: "POST",
  headers: { "content-type": "application/json" },
  body: JSON.stringify({
    model: "gpt-5-mini",
    stream: true,
    messages: [{ role: "user", content: "Draft an incident summary." }]
  })
});

const reader = response.body?.getReader();
if (!reader) throw new Error("Streaming not supported");

const decoder = new TextDecoder();
while (true) {
  const { value, done } = await reader.read();
  if (done) break;
  const chunk = decoder.decode(value, { stream: true });
  process.stdout.write(chunk);
}
```

### Python (Requests)

```python
import requests

response = requests.post(
    "http://localhost:8000/v1/chat/completions",
    json={
        "model": "gpt-5-mini",
        "stream": True,
        "messages": [{"role": "user", "content": "Draft an incident summary."}]
    },
    stream=True,
    timeout=120
)

for line in response.iter_lines():
    if line:
        print(line.decode("utf-8"))
```

## Reverse Proxy and Load Balancer Settings

Streaming requires long-lived connections. Ensure any proxy or load balancer supports:

- Idle timeouts of 60 to 120 seconds or higher
- HTTP/1.1 keep-alive or HTTP/2 support
- Response buffering disabled or minimized

If you use a reverse proxy, set it to pass through chunked responses without buffering.

## Timeouts and Retries

- Client timeouts should be higher than your longest expected response.
- Retries should be disabled for streaming requests unless you support resume logic.
- Consider a fallback to non-streaming if streaming fails.

## Observability

- Track request duration and client disconnects in logs
- Use `/metrics` to monitor latency and error rates
- Enable `json_logs: true` for structured log analysis

## Security Considerations

- Apply the same policy enforcement for streamed and non-streamed requests
- Do not stream responses to untrusted clients without `KORESHIELD_API_KEY`
- Log policy decisions at the proxy to maintain auditability

## Troubleshooting

- Empty stream: confirm the provider supports streaming and `stream: true`
- Broken connections: increase idle timeouts on load balancers
- Delayed chunks: disable response buffering in your proxy

## Related Docs

- Proxy usage in [../integrations/models/openai.mdx](../integrations/models/openai.mdx)
- Monitoring in [../integrations/monitoring/prometheus.mdx](../integrations/monitoring/prometheus.mdx)
